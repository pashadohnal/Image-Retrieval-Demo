{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c1fc8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow\tas tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "import os, random\n",
    "from tensorflow.nn import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09a7746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.jpg', '1.jpg', '10.jpg', '100.jpg', '101.jpg', '102.jpg', '103.jpg', '104.jpg', '105.jpg', '106.jpg', '107.jpg', '108.jpg', '109.jpg', '11.jpg', '110.jpg', '111.jpg', '112.jpg', '113.jpg', '114.jpg', '115.jpg', '116.jpg', '117.jpg', '118.jpg', '119.jpg', '12.jpg', '120.jpg', '121.jpg', '122.jpg', '123.jpg', '124.jpg', '125.jpg', '126.jpg', '127.jpg', '128.jpg', '129.jpg', '13.jpg', '130.jpg', '131.jpg', '132.jpg', '133.jpg', '134.jpg', '135.jpg', '136.jpg', '137.jpg', '138.jpg', '139.jpg', '14.jpg', '140.jpg', '141.jpg', '142.jpg', '143.jpg', '144.jpg', '145.jpg', '146.jpg', '147.jpg', '148.jpg', '149.jpg', '15.jpg', '150.jpg', '151.jpg', '152.jpg', '153.jpg', '154.jpg', '155.jpg', '156.jpg', '157.jpg', '158.jpg', '159.jpg', '16.jpg', '160.jpg', '161.jpg', '162.jpg', '163.jpg', '164.jpg', '165.jpg', '166.jpg', '167.jpg', '168.jpg', '169.jpg', '17.jpg', '170.jpg', '171.jpg', '172.jpg', '173.jpg', '174.jpg', '175.jpg', '176.jpg', '177.jpg', '178.jpg', '179.jpg', '18.jpg', '180.jpg', '181.jpg', '182.jpg', '183.jpg', '184.jpg', '185.jpg', '186.jpg', '187.jpg', '188.jpg', '189.jpg', '19.jpg', '190.jpg', '191.jpg', '192.jpg', '193.jpg', '194.jpg', '195.jpg', '196.jpg', '197.jpg', '198.jpg', '199.jpg', '2.jpg', '20.jpg', '200.jpg', '201.jpg', '202.jpg', '203.jpg', '204.jpg', '205.jpg', '206.jpg', '207.jpg', '208.jpg', '209.jpg', '21.jpg', '210.jpg', '211.jpg', '212.jpg', '213.jpg', '214.jpg', '215.jpg', '216.jpg', '217.jpg', '218.jpg', '219.jpg', '22.jpg', '220.jpg', '221.jpg', '222.jpg', '223.jpg', '224.jpg', '225.jpg', '226.jpg', '227.jpg', '228.jpg', '229.jpg', '23.jpg', '230.jpg', '231.jpg', '232.jpg', '233.jpg', '234.jpg', '235.jpg', '236.jpg', '237.jpg', '238.jpg', '239.jpg', '24.jpg', '240.jpg', '241.jpg', '242.jpg', '243.jpg', '244.jpg', '245.jpg', '246.jpg', '247.jpg', '248.jpg', '249.jpg', '25.jpg', '250.jpg', '251.jpg', '252.jpg', '253.jpg', '254.jpg', '255.jpg', '256.jpg', '257.jpg', '258.jpg', '259.jpg', '26.jpg', '260.jpg', '261.jpg', '262.jpg', '263.jpg', '264.jpg', '265.jpg', '266.jpg', '267.jpg', '268.jpg', '269.jpg', '27.jpg', '270.jpg', '271.jpg', '272.jpg', '273.jpg', '274.jpg', '275.jpg', '276.jpg', '277.jpg', '278.jpg', '279.jpg', '28.jpg', '280.jpg', '281.jpg', '282.jpg', '283.jpg', '284.jpg', '285.jpg', '286.jpg', '287.jpg', '288.jpg', '289.jpg', '29.jpg', '290.jpg', '291.jpg', '292.jpg', '293.jpg', '294.jpg', '295.jpg', '296.jpg', '297.jpg', '298.jpg', '299.jpg', '3.jpg', '30.jpg', '300.jpg', '301.jpg', '302.jpg', '303.jpg', '304.jpg', '305.jpg', '306.jpg', '307.jpg', '308.jpg', '309.jpg', '31.jpg', '310.jpg', '311.jpg', '312.jpg', '313.jpg', '314.jpg', '315.jpg', '316.jpg', '317.jpg', '318.jpg', '319.jpg', '32.jpg', '320.jpg', '321.jpg', '322.jpg', '323.jpg', '324.jpg', '325.jpg', '326.jpg', '327.jpg', '328.jpg', '329.jpg', '33.jpg', '330.jpg', '331.jpg', '332.jpg', '333.jpg', '334.jpg', '335.jpg', '336.jpg', '337.jpg', '338.jpg', '339.jpg', '34.jpg', '340.jpg', '341.jpg', '342.jpg', '343.jpg', '344.jpg', '345.jpg', '346.jpg', '347.jpg', '348.jpg', '349.jpg', '35.jpg', '350.jpg', '351.jpg', '352.jpg', '353.jpg', '354.jpg', '355.jpg', '356.jpg', '357.jpg', '358.jpg', '359.jpg', '36.jpg', '360.jpg', '361.jpg', '362.jpg', '363.jpg', '364.jpg', '365.jpg', '366.jpg', '367.jpg', '368.jpg', '369.jpg', '37.jpg', '370.jpg', '371.jpg', '372.jpg', '373.jpg', '374.jpg', '375.jpg', '376.jpg', '377.jpg', '378.jpg', '379.jpg', '38.jpg', '380.jpg', '381.jpg', '382.jpg', '383.jpg', '384.jpg', '385.jpg', '386.jpg', '387.jpg', '388.jpg', '389.jpg', '39.jpg', '390.jpg', '391.jpg', '392.jpg', '393.jpg', '394.jpg', '395.jpg', '396.jpg', '397.jpg', '398.jpg', '399.jpg', '4.jpg', '40.jpg', '400.jpg', '401.jpg', '402.jpg', '403.jpg', '404.jpg', '405.jpg', '406.jpg', '407.jpg', '408.jpg', '409.jpg', '41.jpg', '410.jpg', '411.jpg', '412.jpg', '413.jpg', '414.jpg', '415.jpg', '416.jpg', '417.jpg', '418.jpg', '419.jpg', '42.jpg', '420.jpg', '421.jpg', '422.jpg', '423.jpg', '424.jpg', '425.jpg', '426.jpg', '427.jpg', '428.jpg', '429.jpg', '43.jpg', '430.jpg', '431.jpg', '432.jpg', '433.jpg', '434.jpg', '435.jpg', '436.jpg', '437.jpg', '438.jpg', '439.jpg', '44.jpg', '440.jpg', '441.jpg', '442.jpg', '443.jpg', '444.jpg', '445.jpg', '446.jpg', '447.jpg', '448.jpg', '449.jpg', '45.jpg', '450.jpg', '451.jpg', '452.jpg', '453.jpg', '454.jpg', '455.jpg', '456.jpg', '457.jpg', '458.jpg', '459.jpg', '46.jpg', '460.jpg', '461.jpg', '462.jpg', '463.jpg', '464.jpg', '465.jpg', '466.jpg', '467.jpg', '468.jpg', '469.jpg', '47.jpg', '470.jpg', '471.jpg', '472.jpg', '473.jpg', '474.jpg', '475.jpg', '476.jpg', '477.jpg', '478.jpg', '479.jpg', '48.jpg', '480.jpg', '481.jpg', '482.jpg', '483.jpg', '484.jpg', '485.jpg', '486.jpg', '487.jpg', '488.jpg', '489.jpg', '49.jpg', '490.jpg', '491.jpg', '492.jpg', '493.jpg', '494.jpg', '495.jpg', '496.jpg', '497.jpg', '498.jpg', '499.jpg', '5.jpg', '50.jpg', '500.jpg', '501.jpg', '502.jpg', '503.jpg', '504.jpg', '505.jpg', '506.jpg', '507.jpg', '508.jpg', '509.jpg', '51.jpg', '510.jpg', '511.jpg', '512.jpg', '513.jpg', '514.jpg', '515.jpg', '516.jpg', '517.jpg', '518.jpg', '519.jpg', '52.jpg', '520.jpg', '521.jpg', '522.jpg', '523.jpg', '524.jpg', '525.jpg', '526.jpg', '527.jpg', '528.jpg', '529.jpg', '53.jpg', '530.jpg', '531.jpg', '532.jpg', '533.jpg', '534.jpg', '535.jpg', '536.jpg', '537.jpg', '538.jpg', '539.jpg', '54.jpg', '540.jpg', '541.jpg', '542.jpg', '543.jpg', '544.jpg', '545.jpg', '546.jpg', '547.jpg', '548.jpg', '549.jpg', '55.jpg', '550.jpg', '551.jpg', '552.jpg', '553.jpg', '554.jpg', '555.jpg', '556.jpg', '557.jpg', '558.jpg', '559.jpg', '56.jpg', '560.jpg', '561.jpg', '562.jpg', '563.jpg', '564.jpg', '565.jpg', '566.jpg', '567.jpg', '568.jpg', '569.jpg', '57.jpg', '570.jpg', '571.jpg', '572.jpg', '573.jpg', '574.jpg', '575.jpg', '576.jpg', '577.jpg', '578.jpg', '579.jpg', '58.jpg', '580.jpg', '581.jpg', '582.jpg', '583.jpg', '584.jpg', '585.jpg', '586.jpg', '587.jpg', '588.jpg', '589.jpg', '59.jpg', '590.jpg', '591.jpg', '592.jpg', '593.jpg', '594.jpg', '595.jpg', '596.jpg', '597.jpg', '598.jpg', '599.jpg', '6.jpg', '60.jpg', '600.jpg', '601.jpg', '602.jpg', '603.jpg', '604.jpg', '605.jpg', '606.jpg', '607.jpg', '608.jpg', '609.jpg', '61.jpg', '610.jpg', '611.jpg', '612.jpg', '613.jpg', '614.jpg', '615.jpg', '616.jpg', '617.jpg', '618.jpg', '619.jpg', '62.jpg', '620.jpg', '621.jpg', '622.jpg', '623.jpg', '624.jpg', '625.jpg', '626.jpg', '627.jpg', '628.jpg', '629.jpg', '63.jpg', '630.jpg', '631.jpg', '632.jpg', '633.jpg', '634.jpg', '635.jpg', '636.jpg', '637.jpg', '638.jpg', '639.jpg', '64.jpg', '640.jpg', '641.jpg', '642.jpg', '643.jpg', '644.jpg', '645.jpg', '646.jpg', '647.jpg', '648.jpg', '649.jpg', '65.jpg', '650.jpg', '651.jpg', '652.jpg', '653.jpg', '654.jpg', '655.jpg', '656.jpg', '657.jpg', '658.jpg', '659.jpg', '66.jpg', '660.jpg', '661.jpg', '662.jpg', '663.jpg', '664.jpg', '665.jpg', '666.jpg', '667.jpg', '668.jpg', '669.jpg', '67.jpg', '670.jpg', '671.jpg', '672.jpg', '673.jpg', '674.jpg', '675.jpg', '676.jpg', '677.jpg', '678.jpg', '679.jpg', '68.jpg', '680.jpg', '681.jpg', '682.jpg', '683.jpg', '684.jpg', '685.jpg', '686.jpg', '687.jpg', '688.jpg', '689.jpg', '69.jpg', '690.jpg', '691.jpg', '692.jpg', '693.jpg', '694.jpg', '695.jpg', '696.jpg', '697.jpg', '698.jpg', '699.jpg', '7.jpg', '70.jpg', '700.jpg', '701.jpg', '702.jpg', '703.jpg', '704.jpg', '705.jpg', '706.jpg', '707.jpg', '708.jpg', '709.jpg', '71.jpg', '710.jpg', '711.jpg', '712.jpg', '713.jpg', '714.jpg', '715.jpg', '716.jpg', '717.jpg', '718.jpg', '719.jpg', '72.jpg', '720.jpg', '721.jpg', '722.jpg', '723.jpg', '724.jpg', '725.jpg', '726.jpg', '727.jpg', '728.jpg', '729.jpg', '73.jpg', '730.jpg', '731.jpg', '732.jpg', '733.jpg', '734.jpg', '735.jpg', '736.jpg', '737.jpg', '738.jpg', '739.jpg', '74.jpg', '740.jpg', '741.jpg', '742.jpg', '743.jpg', '744.jpg', '745.jpg', '746.jpg', '747.jpg', '748.jpg', '749.jpg', '75.jpg', '750.jpg', '751.jpg', '752.jpg', '753.jpg', '754.jpg', '755.jpg', '756.jpg', '757.jpg', '758.jpg', '759.jpg', '76.jpg', '760.jpg', '761.jpg', '762.jpg', '763.jpg', '764.jpg', '765.jpg', '766.jpg', '767.jpg', '768.jpg', '769.jpg', '77.jpg', '770.jpg', '771.jpg', '772.jpg', '773.jpg', '774.jpg', '775.jpg', '776.jpg', '777.jpg', '778.jpg', '779.jpg', '78.jpg', '780.jpg', '781.jpg', '782.jpg', '783.jpg', '784.jpg', '785.jpg', '786.jpg', '787.jpg', '788.jpg', '789.jpg', '79.jpg', '790.jpg', '791.jpg', '792.jpg', '793.jpg', '794.jpg', '795.jpg', '796.jpg', '797.jpg', '798.jpg', '799.jpg', '8.jpg', '80.jpg', '800.jpg', '801.jpg', '802.jpg', '803.jpg', '804.jpg', '805.jpg', '806.jpg', '807.jpg', '808.jpg', '809.jpg', '81.jpg', '810.jpg', '811.jpg', '812.jpg', '813.jpg', '814.jpg', '815.jpg', '816.jpg', '817.jpg', '818.jpg', '819.jpg', '82.jpg', '820.jpg', '821.jpg', '822.jpg', '823.jpg', '824.jpg', '825.jpg', '826.jpg', '827.jpg', '828.jpg', '829.jpg', '83.jpg', '830.jpg', '831.jpg', '832.jpg', '833.jpg', '834.jpg', '835.jpg', '836.jpg', '837.jpg', '838.jpg', '839.jpg', '84.jpg', '840.jpg', '841.jpg', '842.jpg', '843.jpg', '844.jpg', '845.jpg', '846.jpg', '847.jpg', '848.jpg', '849.jpg', '85.jpg', '850.jpg', '851.jpg', '852.jpg', '853.jpg', '854.jpg', '855.jpg', '856.jpg', '857.jpg', '858.jpg', '859.jpg', '86.jpg', '860.jpg', '861.jpg', '862.jpg', '863.jpg', '864.jpg', '865.jpg', '866.jpg', '867.jpg', '868.jpg', '869.jpg', '87.jpg', '870.jpg', '871.jpg', '872.jpg', '873.jpg', '874.jpg', '875.jpg', '876.jpg', '877.jpg', '878.jpg', '879.jpg', '88.jpg', '880.jpg', '881.jpg', '882.jpg', '883.jpg', '884.jpg', '885.jpg', '886.jpg', '887.jpg', '888.jpg', '889.jpg', '89.jpg', '890.jpg', '891.jpg', '892.jpg', '893.jpg', '894.jpg', '895.jpg', '896.jpg', '897.jpg', '898.jpg', '899.jpg', '9.jpg', '90.jpg', '900.jpg', '901.jpg', '902.jpg', '903.jpg', '904.jpg', '905.jpg', '906.jpg', '907.jpg', '908.jpg', '909.jpg', '91.jpg', '910.jpg', '911.jpg', '912.jpg', '913.jpg', '914.jpg', '915.jpg', '916.jpg', '917.jpg', '918.jpg', '919.jpg', '92.jpg', '920.jpg', '921.jpg', '922.jpg', '923.jpg', '924.jpg', '925.jpg', '926.jpg', '927.jpg', '928.jpg', '929.jpg', '93.jpg', '930.jpg', '931.jpg', '932.jpg', '933.jpg', '934.jpg', '935.jpg', '936.jpg', '937.jpg', '938.jpg', '939.jpg', '94.jpg', '940.jpg', '941.jpg', '942.jpg', '943.jpg', '944.jpg', '945.jpg', '946.jpg', '947.jpg', '948.jpg', '949.jpg', '95.jpg', '950.jpg', '951.jpg', '952.jpg', '953.jpg', '954.jpg', '955.jpg', '956.jpg', '957.jpg', '958.jpg', '959.jpg', '96.jpg', '960.jpg', '961.jpg', '962.jpg', '963.jpg', '964.jpg', '965.jpg', '966.jpg', '967.jpg', '968.jpg', '969.jpg', '97.jpg', '970.jpg', '971.jpg', '972.jpg', '973.jpg', '974.jpg', '975.jpg', '976.jpg', '977.jpg', '978.jpg', '979.jpg', '98.jpg', '980.jpg', '981.jpg', '982.jpg', '983.jpg', '984.jpg', '985.jpg', '986.jpg', '987.jpg', '988.jpg', '989.jpg', '99.jpg', '990.jpg', '991.jpg', '992.jpg', '993.jpg', '994.jpg', '995.jpg', '996.jpg', '997.jpg', '998.jpg', '999.jpg']\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "folder_path = \"image.orig\"\n",
    "\n",
    "for item in os.listdir(folder_path):\n",
    "    item_path = os.path.join(folder_path, item)  # Get full path\n",
    "    if item.split(\".\")[1] == \"jpg\":\n",
    "        files += [f\"{item}\"]\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "688bf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = []\n",
    "labels = [\"africa\", \"beach\", \"ancient\", \"bus\", \"dinosaur\", \"elephant\",  \"flower\"    , \"horse\", \"mountain\", \"food\"]\n",
    "train_labels = []\n",
    "\n",
    "# Target size \n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "\n",
    "for file in files:\n",
    "    img = cv.imread(\"image.orig/\" + file)\n",
    "    if img is not None:\n",
    "        img = cv.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to 32x32\n",
    "        images.append(img)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    train_labels += [i] * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "45199d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set after augmentation: 3200 images\n",
      "Testing set: 200 images\n"
     ]
    }
   ],
   "source": [
    "# --- Helper functions for augmentation ---\n",
    "def rotate_image(img, angle_range=(-15, 15)):\n",
    "    angle = np.random.uniform(*angle_range)\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
    "    return cv.warpAffine(img, M, (w, h))\n",
    "\n",
    "def random_flip(img):\n",
    "    if np.random.rand() > 0.5:\n",
    "        img = cv.flip(img, 1)  # horizontal flip\n",
    "    return img\n",
    "\n",
    "def random_brightness(img, factor=0.2):\n",
    "    # Ensure image is 3-channel uint8\n",
    "    if len(img.shape) != 3 or img.shape[2] != 3:\n",
    "        img = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "\n",
    "    img = img.astype(np.uint8)\n",
    "    hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    h, s, v = cv.split(hsv)\n",
    "    # Ensure v is uint8\n",
    "    v = np.clip(v.astype(np.float32) * (1 + np.random.uniform(-factor, factor)), 0, 255).astype(np.uint8)\n",
    "    hsv = cv.merge([h, s, v])\n",
    "    img_out = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    return img_out\n",
    "\n",
    "def augment_image(img):\n",
    "    img = rotate_image(img)\n",
    "    img = random_flip(img)\n",
    "    img = random_brightness(img)\n",
    "    return img\n",
    "\n",
    "# --- Shuffle dataset ---\n",
    "combined = list(zip(images, train_labels))\n",
    "random.shuffle(combined)\n",
    "images, train_labels = zip(*combined)\n",
    "images = list(images)\n",
    "train_labels = list(train_labels)\n",
    "\n",
    "# --- Split into training and testing sets ---\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(images) * split_ratio)\n",
    "\n",
    "original_train_images = images[:split_index]\n",
    "train_labels_final = train_labels[:split_index]\n",
    "\n",
    "test_images = np.array(images[split_index:])\n",
    "test_labels_final = np.array(train_labels[split_index:])\n",
    "\n",
    "# --- Augment training images to increase dataset size ---\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "AUG_PER_IMAGE = 3  # number of augmented copies per original image\n",
    "\n",
    "for img, label in zip(original_train_images, train_labels_final):\n",
    "    augmented_images.append(img)          # keep original\n",
    "    augmented_labels.append(label)\n",
    "    for _ in range(AUG_PER_IMAGE):\n",
    "        aug_img = augment_image(img)\n",
    "        augmented_images.append(aug_img)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "# Convert to NumPy arrays and normalize\n",
    "train_images = np.array(augmented_images).astype(np.float32) / 255.0\n",
    "train_labels_final = np.array(augmented_labels)\n",
    "\n",
    "# Test set remains unchanged and normalized\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "print(f\"Training set after augmentation: {len(train_images)} images\")\n",
    "print(f\"Testing set: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2c30df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(4, (3,3), activation='relu', input_shape=(128,128,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(8, (5,5), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(16, (5 ,5), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(16, (5 ,5), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c9f96097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.0687 - loss: 3.9729 - val_accuracy: 0.1500 - val_loss: 2.2488\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.1313 - loss: 2.6071 - val_accuracy: 0.2550 - val_loss: 2.0970\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.2033 - loss: 2.2902 - val_accuracy: 0.3850 - val_loss: 1.9083\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.2420 - loss: 2.1185 - val_accuracy: 0.3850 - val_loss: 1.7847\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3232 - loss: 1.9176 - val_accuracy: 0.4250 - val_loss: 1.6696\n",
      "Epoch 6/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3386 - loss: 1.8246 - val_accuracy: 0.4300 - val_loss: 1.6077\n",
      "Epoch 7/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3611 - loss: 1.7192 - val_accuracy: 0.4150 - val_loss: 1.5915\n",
      "Epoch 8/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3805 - loss: 1.6550 - val_accuracy: 0.4450 - val_loss: 1.4803\n",
      "Epoch 9/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4223 - loss: 1.5687 - val_accuracy: 0.4650 - val_loss: 1.5278\n",
      "Epoch 10/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4019 - loss: 1.5527 - val_accuracy: 0.4750 - val_loss: 1.4114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1995ba2e4d0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels_final, epochs=10, batch_size=32, \n",
    "                    validation_data=(test_images, test_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8bd12273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: beach\n",
      "2: mountain\n",
      "3: food\n",
      "4: dinosaur\n",
      "5: flower\n",
      "6: horse\n",
      "7: elephant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You chose: beach.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "The most similar image is image.orig\\264.jpg with distance 15.7288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.autocall.ZMQExitAutocall at 0x1992ed19c10>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieval(model):\n",
    "    # --- Choose query image ---\n",
    "    print(\"1: beach\\n2: mountain\\n3: food\\n4: dinosaur\\n5: flower\\n6: horse\\n7: elephant\")\n",
    "    choice = input(\"Type the number to choose a category: \")\n",
    "\n",
    "    query_file_map = {\n",
    "        '1': 'beach.jpg',\n",
    "        '2': 'mountain.jpg',\n",
    "        '3': 'food.jpg',\n",
    "        '4': 'dinosaur.jpg',\n",
    "        '5': 'flower.jpg',\n",
    "        '6': 'horse.jpg',\n",
    "        '7': 'elephant.jpg'\n",
    "    }\n",
    "\n",
    "    if choice not in query_file_map:\n",
    "        print(\"Invalid choice\")\n",
    "        return\n",
    "\n",
    "    src_path = os.path.join(\"image.query\", query_file_map[choice])\n",
    "    src_input = cv.imread(src_path)\n",
    "    src_input_resized = cv.resize(src_input, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    src_input_resized = src_input_resized.astype(np.float32) / 255.0\n",
    "    src_input_resized = np.expand_dims(src_input_resized, axis=0)  # batch dimension\n",
    "\n",
    "    print(f\"You chose: {query_file_map[choice]}\")\n",
    "\n",
    "    cv.imshow(\"Query\", cv.resize(src_input, (256, 256)))\n",
    "\n",
    "    # --- Load database images ---\n",
    "    database_dir = \"image.orig\"\n",
    "    database_files = sorted(glob(os.path.join(database_dir, \"*.jpg\")))\n",
    "\n",
    "    db_images = []\n",
    "    for file in database_files:\n",
    "        img = cv.imread(file)\n",
    "        img = cv.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        db_images.append(img)\n",
    "    db_images = np.array(db_images)\n",
    "\n",
    "    # --- Predict logits ---\n",
    "    query_logit = model.predict(src_input_resized)  # shape: (1, 10)\n",
    "    db_logits = model.predict(db_images)            # shape: (num_images, 10)\n",
    "\n",
    "    # --- Compute Euclidean distances ---\n",
    "    distances = np.linalg.norm(db_logits - query_logit, axis=1)\n",
    "    closest_idx = np.argmin(distances)\n",
    "    closest_file = database_files[closest_idx]\n",
    "\n",
    "    print(f\"The most similar image is {closest_file} with distance {distances[closest_idx]:.4f}\")\n",
    "\n",
    "    # --- Show result ---\n",
    "    closest_img = cv.imread(closest_file)\n",
    "    cv.imshow(\"Closest Match\", cv.resize(closest_img, (256, 256)))\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "    exit\n",
    "    \n",
    "retrieval(model)\n",
    "exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
